### 现有专线模型：
<img width="838" alt="图片 1" src="https://github.com/nieting1997/work_1/assets/90097659/eb4e67d2-4f06-40c7-9855-a42aee819c2e">

* 云上虚机群互通和专线流量使用同一个虚拟网关。专线接入带宽大，若网关无法支撑大流量出现频繁丢包，会影响云上虚机群其他业务。
* 受限于虚拟网关，现云上虚机群只支持两两互备。现IDC机房只能有两个接入点。(另一个团队开发该路由转发功能，设备组只能支持两个。通过keep-alive探活。)
* 业务之外：
  * 现使用转发面均为软转发，基于dpdk实现。业界已经有可编程交换芯片，通过硬件转发，在带宽、转发性能等方面远超现在的路由集群。
  * 可编程交换机自研，取代原有软转发服务器后，成本可下降x%。
  * 可编程交换机可实现流量统计，限速等等定制化功能。

### 基于可编程交换机的模型：
<img width="478" alt="图片 2" src="https://github.com/nieting1997/work_1/assets/90097659/d4d64abf-3fca-4b04-8ee6-770c608102c3">

* 位置侧 vxlan port：记录通向vpc侧的vxlan隧道
* association port：将网关vrf与vpc vrf绑定
* 非位置侧 vxlan port：记录用户侧vxlan隧道
* vxlan gateway port：记录出云vxlan隧道

设计之后：
* 专线网关和VPC网关解耦。
* 专线网关之间通过BGP进行通信，利用BGP协议而不是设备组进行互备。理论上，可支持多台互备，解决痛点。
* 设计支持多层vrf，应对不同vpc内部网段重合的问题。
* 其中，位置侧vxlan port和非位置侧vxlan port区别在于非位置侧会携带vlan信息。实际存在一条物理专线通过不同vlan接入多个租户的情况，也存在同一个租户通过不同vlan接入不同vpc的情况。此种情况会有多个网关vrf。
* 出云流量只需关注目的vtep地址，这样能省略很多条目的vetep相同但源vtep不同的表项，节约空间。因此，入云出云不是一条线路。

### 底层表项
业务层所有的模型建模转化到底层，都是一个个表项。  
表项有三种常见的：  
* 线性表。最常见的表，通过index来寻址，速度快。(类似数组)
* 精确匹配表。算法表，通过字段精确匹配，比较费时。
* 最长前缀匹配表。算法表，通过最长匹配算法匹配，费时。
对于业务模型，其对应表项如下(此处信息做模糊化处理)：
* 客户入云：  
1）包达到网关，通过精确匹配表获取入云要通过的路由表(gw vrf)。  
2）通过最长前缀匹配表，利用vrf和ip目的地址获取出口的信息，对应具体的位置表(vpc vrf)。  
3）位置表通过线性表对报文做封装，并给出下一跳地址。  
* 客户出云：  
1）包达到网关，通过精确匹配表获取出云要通过的路由表(gw vrf)  
2）通过最长前缀匹配表，利用vrf和ip目的地址获取出口信息，对应vxlan port信息的index。  
3）通过线性表，利用index对报文做封装，并给出下一跳地址。

### 个人工作
* agent模块owner，大概共3w+代码，使用golang完成。
* 除模型开发外，通用层面进行了如下优化：
 * 使用异步日志，节约日志落盘时间。
 * 使用数据库连接池，节约数据查询、保存时间。
 * 同时提供grpc、kafka两种接口，应对单播、广播场景。
 * 收到信息后，采用协程异步方法处理业务逻辑，防止同步阻塞等待。
